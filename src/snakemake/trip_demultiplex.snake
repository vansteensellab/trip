import getpass
import datetime
import inspect
import os
import re
import yaml

filename = inspect.getframeinfo(inspect.currentframe()).filename
path = os.path.dirname(os.path.abspath(filename))


# user = getpass.getuser()
# date = datetime.datetime.now()
# date = '%i%0.2i%0.2i' % (date.year, date.month, date.day)
# OUTDIR = ''.join((user[0], user[2], date, '_', config["dir_suffix"]))
OUTDIR = config['outdir']
if 'extract' in config:
    TIMING = glob_wildcards(config["extract"]["timing"])[0]
# TYPE_LIST = ['mapping', 'gDNA', 'cDNA', 'spike']
# TYPE_LIST = [read_type for read_type in TYPE_LIST if read_type in config]

# if 'groups' in config:
#     group_name_vec = [group[0] for group in config['groups']]
#     replicate_dict = {}
#     if 'replicate' in group_name_vec:
#         index = group_name_vec.index('replicate')
#         for name in config['input_file']['gDNA'].keys():
#             if 'spike' in config['input_file']:
#                 file_name = '%s/cDNA/%s.cpm.gDNA/spike' % (OUTDIR, name)
#             else:
#                 file_name = '%s/cDNA/%s.cpm.gDNA' % (OUTDIR, name)
#             name_split = name.split('_')
#             if index < len(name_split):
#                 name_vec = [name_split[i] for i in range(0, len(name_split))
#                             if i != index]
#                 mean_name = '_'.join(name_vec)
#                 if mean_name in replicate_dict:
#                     replicate_dict[mean_name].append(file_name)
#                 else:
#                     replicate_dict[mean_name] = [file_name]



def get_rep_input(config, datatype):
    index_dict = config['starcode_params']['pick'][datatype]
    for name in index_dict:
        for bio in index_dict[name]:
            for i in range(0,len(index_dict[name][bio])):
                yield('%s_%s_r%i' % (name, bio, i + 1))

def get_comb_input(config, datatype):
    index_dict = config['starcode_params']['pick'][datatype]
    for name in index_dict:
        for bio in index_dict[name]:
            yield('%s_%s' % (name, bio))



if 'chip_config' in config:
    with open(config['chip_config']) as c:
        config.update(yaml.load(c))
        config['bed_summary'] = {}
        for name in get_comb_input(config, 'mapping'):
            for window in config['signal_window']:
                label = '%s_%i' % (name, window)
                config['bed_summary'][label] = '%s/region/%s.bed' % (OUTDIR,
                                                                     label)
            config['bed_summary'][name] = '%s/sites/%s.bed' % (OUTDIR, name)


    with open(config['chip_align_config']) as c:
        config.update(yaml.load(c))
    ruleorder: input_demulti_single > download_single_sra
    ruleorder: input_demulti_paired > download_single_sra
    include: config["chip_snake"]



def get_annotation_input(config, outdir):
    calls_regex = '%s/calls/%s_%s.txt'
    signal_regex = '%s/signal/%s_%s_%i.txt'
    dam_regex = '%s/dam/%s_%s_%i.txt'
    chip_regex = '%s/chip/means/%s_%i_%s_%s_%s.txt'
    peak_regex = '%s/chip/nearest_peaks/%s_%s_%s_%s.txt'
    domain_regex = '%s/chip/%s/%s_%s_%s_%s.txt'
    hic_regex = '%s/hic_compaction/%s_%i_%i.txt'
    in_list = []
    for name in get_comb_input(config, 'mapping'):
        if 'calls' in config['annotation']:
            for calls in config['annotation']['calls']:
                yield(calls_regex % (outdir, name, calls))
        if 'signal' in config['annotation']:
            for signal_type in config['annotation']['signal']:
                for window in config['signal_window']:
                    yield(signal_regex % (outdir, name, signal_type, window))
        if 'DAM' in config['annotation']:
            for dam_type in config['annotation']['DAM']:
                for window in config['dam_window']:
                    yield(dam_regex % (outdir, name, dam_type, window))
        if 'hic_compaction' in config['annotation']:
            for max_dist in config['annotation']['hic_compaction']['max_dist']:
                for window in config['annotation']['hic_compaction']['window']:
                    yield(hic_regex % (outdir, name, max_dist, window))
        if config['annotation']['chip']:
            for exp_list in iter_expirements(config, config['celltype']):
                ct, experiment, target, sample, info = exp_list
                for window in config['signal_window']:
                    yield(chip_regex % (outdir, name, window, experiment,
                                        target, sample))
        if config['annotation']['chip_peaks']:
            for exp_list in iter_expirements(config, config['celltype']):
                ct, experiment, target, sample, info = exp_list
                yield(peak_regex % (outdir, name, experiment,
                                    target, sample))
        if config['annotation']['chip_domains']:
            for exp_list in iter_expirements(config, config['celltype']):
                ct, experiment, target, sample, info = exp_list
                yield(domain_regex % (outdir, 'domains_call', name, experiment,
                                      target, sample))
                yield(domain_regex % (outdir, 'nearest_domains', name, experiment,
                                      target, sample))
        if 'lad_count' in config:
            yield('%s/lad_count/%s.txt' % (outdir, name))



def _get_input(config, datatype, outdir):
    if datatype == 'mapping':
        if 'annotation' in config:
            for ip in get_annotation_input(config, outdir):
                yield(ip)
        else:
            for name in get_rep_input(config, 'mapping'):
                yield('%s/mapping/%s.2.table' % (outdir, name))
    elif datatype == 'cDNA':
        for name in get_rep_input(config, 'cDNA'):
            yield('%s/cDNA/%s.normalized' % (outdir, name))
    elif datatype == 'gDNA':
        for name in get_rep_input(config, 'gDNA'):
            yield('%s/gDNA/%s.starcode.count' % (outdir, name))
    elif datatype == 'pDNA':
        for name in get_rep_input(config, 'pDNA'):
            yield('%s/pDNA/%s.starcode.count' % (outdir, name))

def get_input(config, datatype, outdir):
    if datatype == 'all':
        for datatype in config['starcode_params']['pick']:
            for ip in _get_input(config, datatype, outdir):
                yield(ip)
    else:
        for ip in _get_input(config, datatype, outdir):
            yield(ip)

rule all:
    input:
        [ip for ip in get_input(config, 'all', OUTDIR)]



if 'cDNA' in config['structure']['pick']:
    rule cdna_only:
        input:
            [ip for ip in get_input(config, 'cDNA', OUTDIR)]

if 'gDNA' in config['structure']['pick']:
    rule gdna_only:
        input:
            [ip for ip in get_input(config, 'gDNA', OUTDIR)]

if 'pDNA' in config['structure']['pick']:
    rule pdna_only:
        input:
            [ip for ip in get_input(config, 'pDNA', OUTDIR)]


def get_dam_label(config, wildcards):
    dam_dict = config['annotation']["DAM"][wildcards.experiment]
    R = len(dam_dict['experiment']['bw'])
    for i in range(0,R):
        yield('exp_%i' % i)
    for i in range(0,R):
        yield('ctrl_%i' % i)


def get_dam_input(config, wildcards, suffix):
    dam_dict = config['annotation']["DAM"][wildcards.experiment]
    for exp in dam_dict['experiment']:
        yield('%s/%s.%s' % (config['dam_dir'], exp, suffix))
    for dam in dam_dict['control']:
        yield('%s/%s.%s' % (config['dam_dir'], dam, suffix))
if 'lad_count' in config:
    rule count_lad:
        input:
            a="{outdir}/sites/{bed}.bed",
            b=config['lad_count']
        output:
            "{outdir}/lad_count/{bed}.txt"
        shell:
            "bedtools intersect -c -a {input.a} -b {input.b} > {output}"

rule dam_means:
    input:
        bed="{outdir}/sites/{bed}.bed",
        counts=lambda wildcards: get_dam_input(config, wildcards, 'counts.txt.gz')
    output:
        '{outdir}/dam/{bed}_{experiment}_{window}.txt'
    params:
        window='{window}'
    run:
        input_counts = ','.join(input.counts)
        shell("{path}/scripts/compute_dam_region.R --window {params.window} "
              "                                    --bed {input.bed}"
              "                                    --counts {input_counts}"
              "                                    --out {output}")



def get_signal_dict(config):
    if 'signal' in config['annotation']:
        return(config['annotation']['signal']['bw'])
    else:
        return('')

def get_signal_bw(config):
    if 'signal' in config['annotation']:
        return(config['annotation']['signal']['bw'].values())
    else:
        return('')


def get_call(config, wildcards):
    file_list = config['annotation']['calls'][wildcards.calls]
    return('/'.join((config['dam_dir'], file_name)) for file_name in file_list)


if 'annotation' in config:
    rule bw_signal:
        input:
            bed="{outdir}/region/{name}_{bio}_{window}.bed",
            bw=get_signal_bw(config)
        output:
            "{outdir}/signal/{name}_{bio}_bw_{window}.npz",
            "{outdir}/signal/{name}_{bio}_bw_{window}.txt"
        params:
            bw_dict=get_signal_dict(config)
        threads:
            5
        run:
            key_list = list(params.bw_dict.keys())
            bw_str = ' '.join([params.bw_dict[key] for key in key_list])
            label = ' '.join(key_list)
            command = ("multiBigwigSummary BED-file -p %s"
                       "                            --BED %s"
                       "                            -l %s"
                       "                            -b %s"
                       "                            -out %s"
                       "                            --outRawCounts %s")
            shell(command % (threads, input.bed, label, bw_str,
                             output[0], output[1]))
    rule hic_compaction:
        input:
            bed="{outdir}/sites/{name}.bed",
            indices=config['annotation']['hic_compaction']['indices'],
            signal=config['annotation']['hic_compaction']['signal']
        output:
            '{outdir}/hic_compaction/{name}_{max_dist}_{window}.txt'
        params:
            max_dist='{max_dist}',
            window='{window}'
        threads:
            5
        shell:
            "{path}/scripts/hic_compaction.R -b {input.bed}"
            "                                -i {input.indices}"
            "                                -s {input.signal}"
            "                                -w {params.window}"
            "                                -d {params.max_dist}"
            "                                -o {output}"
    rule calls:
        input:
            "{outdir}/sites/{name}_{bio}.bed",
            lambda wildcards: get_call(config, wildcards)
        output:
            "{outdir}/calls/{name}_{bio}_{calls}.txt"
        shell:
            "{path}/scripts/domain_call.R --regions {input[0]}"
            "                             --domains {input[1]}"
            "                             --out {output}"

def get_region_input(config, wildcards):
    map_dict = config['starcode_params']['pick']['mapping']
    r_len = len(map_dict[wildcards.name][wildcards.bio])
    for i in range(0, r_len):
        yield("%s/mapping/%s_%s_r%i.2.table" % (wildcards.outdir,
                                                wildcards.name,
                                                wildcards.bio, i + 1))
rule region_bed:
    input:
        "{outdir}/sites/{name}_{bio}.bed"
    output:
        "{outdir}/region/{name}_{bio}_{window}.bed"
    params:
        window= lambda wildcards: wildcards.window
    shell:
        "awk -vOFS='\t' '{{"
        "    print $1, $2-({params.window}/2), $3+({params.window}/2), $4, $5, $6"
        "}}' {input} > {output}"


rule map_bed:
    input:
        lambda wildcards: get_region_input(config, wildcards)
    output:
        "{outdir}/sites/{name}_{bio}.bed"
    shell:
        "{path}/scripts/map_bed.R {input} > {output}"

if 'mapping' in config['structure']['pick']:
    rule parse_sam:
        input:
            bam='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.bam',
            count='{outdir}/mapping/{name}_{bio}_r{rep}.starcode.count',
            mutated='{outdir}/mapping/{name}_{bio}_r{rep}.genuine.cut'
        output:
            bed='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.bed',
            table='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.table',
            stats='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.parse_stat.table',
            length='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.length.table',
            remap_fq='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.remap.fastq.gz',
            remap='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.remap.bam'
        wildcard_constraints:
            name="[^_]+",
            bio="[^_]+",
            num="\d+",
            rep="\d+"
        params:
            bowtie_index = config['bowtie']['index'],
            options = lambda wildcards: config['bowtie']['options'][wildcards.num],
            max_dist = lambda wildcards: config['max_dist'][wildcards.num]
        threads: 3
        script:
            'scripts/parse_sam.py'

    rule align:
        input:
            '{outdir}/mapping/{name}_{bio}_r{rep}.{num}.fastq.gz'
        output:
            bam='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.bam'
        params:
            bowtie_index=config['bowtie']['index'],
            options=config['bowtie']['options'],
            num='{num}'
        wildcard_constraints:
            num="\d+",
            rep="\d+"
        threads: 10
        log:
            '{outdir}/mapping/align.{name}.{num}.log'
        run:
            gunzip = "gunzip -c {input}"
            ## filter for read length
            awk = ("awk '{{"
                   "       step=NR%4;"
                   "       if (step==0 && length(a[2])>6){{"
                   "           for (i in a){{"
                   "               print a[i]"
                   "           }}"
                   "           print $0;"
                   "           hit+=1;"
                   "       }} else if (step!=0){{"
                   "           a[step]=$0;"
                   "       }} else {{"
                   "           short+=1"
                   "       }}"
                   "}} END {{"
                   "print \"filtering before mapping with bowtie2:\" > \"{log}\"; "
                   "printf \"%i\\treads; of these:\\n\", hit+short > \"{log}\"; "
                   "printf \"  %i (%2.2f%%) were long enough (> 6bp)\\n\", hit, hit/(hit+short)*100 > \"{log}\"; "
                   "printf \"  %i (%2.2f%%) were too short (<= 6bp)\\n\\n\", short, short/(hit+short)*100 > \"{log}\"; "
                   "print \"stats from bowtie2:\" > \"{log}\"; "
                   "}}'")
            options = ' '.join(params.options[params.num])
            bowtie = 'bowtie2 -p {threads} %s -x %s -U - ' % (options,
                                                              params.bowtie_index)
            samToBam = 'samtools view -@ {threads} -Sb - 1> {output.bam} 2>> {log}'
            shell('%s | %s | %s | %s' % (gunzip, awk, bowtie, samToBam))

    rule split_mapping:
        input:
            bam=expand('{outdir}/mapping/{{num}}.bam', outdir=OUTDIR)
        output:
            '{outdir}/mapping/{num}.genuine.bam'
            '{outdir}/mapping/{num}.genuine.cut.bam'
            '{outdir}/mapping/{num}.unmapped.bam'


###############################################################################
##+++++++++++++++++++++++++++++ mean expression +++++++++++++++++++++++++++++##
###############################################################################


# rule mean_exp:
#     input:
#         lambda wildcards: replicate_dict[wildcards.mean_name]
#     output:
#         '%s/cDNA/{mean_name}.mean',
#         '%s/cDNA/{mean_name}.mean.cut'
#     run:
#         cpm_dict = {}
#         mean_file = open('{output[0]}', 'w')
#         mean_cut_file = open('{output[1]}', 'w')
#         for input_file in snakemake.input:
#             with open(input_file) as file_in:
#                 for line in file_in.readlines():
#                     norm_cpm, barcode = line.strip().split()
#                     if barcode in cpm_dict:
#                         cpm_dict[barcode][input_file] = float(norm_cpm)
#                     else:
#                         cpm_dict[barcode] = {input_file: float(norm_cpm)}
#         for barcode in cpm_dict:
#             if len(cpm_dict[barcode]) == len(snakemake.input):
#                 mean = sum(cpm_dict[barcode].values())/len(snakemake.input)
#                 mean_file.write('%f\t%s' % (mean, barcode))
#         mean_file.close()
#         mean_cut_file.close()




###############################################################################
##++++++++++++++++++++++ calculate counts per million +++++++++++++++++++++++##
###############################################################################
#
# rule cpm:
#     input:
#         expand('{outdir}/{read_type}.{{name}}.starcode.count', outdir=OUTDIR,
#                read_type = ('cDNA', 'gDNA', 'spike'))
#     output:
#         '{outdir}/{read_type}.{name}.cpm'
#     shell:
#         "awk '{{arr[$2] = $1; sum += $1}}"
#         "END{{for (bc in arr){{print arr[bc]/sum*1000000\"\t\"bc}}}}'"
#         "< {input} > {output}"

if 'spike' in config['structure']['pick']:
    rule normalize_mean_expression:
        input:
            expand('{outdir}/cDNA/{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/gDNA/{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/spike/{{name}}.starcode.count', outdir=OUTDIR)
        output:
            '{outdir}/cDNA/{name}.normalized'
        params:
            path
        shell:
            'Rscript {params}/scripts/normalize.R {input} {output}'
else:
    rule normalize_mean_expression:
        input:
            expand('{outdir}/cDNA/{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/gDNA/{{name}}.starcode.count', outdir=OUTDIR)
        output:
            '{outdir}/cDNA/{name}.normalized'
        params:
            path
        shell:
            'Rscript {params}/scripts/normalize.R {input} {output}'

rule normalize_polyA:
    input:
        lambda wildcards: config['input_file']['polyA'][wildcards.name][0],
        expand('{outdir}/polyA/{{name}}.starcode.count', outdir=OUTDIR)
    output:
        '{outdir}/polyA/{name}.normalized'
    shell:
        "total_reads=$(($(gunzip -c {input[0]} | wc -l) / 4));"
        "awk -v total_reads=\"$total_reads\" '{{"
        "    norm=$2/total_reads * 1000000;"
        "    print $0\"\t\"norm"
        "}}' {input[1]} > {output}"



###############################################################################
##++++++++++++++++++++++++ select genuine barcodes ++++++++++++++++++++++++++##
###############################################################################

rule starcode_ref:
    input:
        '{outdir}/{data_type}/{name}_{bio}.raw.count'
    output:
        gen='{outdir}/{data_type}/{name}_{bio}.starcode.count',
        mut='{outdir}/{data_type}/{name}_{bio}.genuine.cut',
        count='{outdir}/{data_type}/{name}_{bio}.count.cut'
    wildcard_constraints:
        name="[^_]+",
        bio="[^_]+"
    params:
        lev_dist = config['lev_dist'],
        use_other = False,
        count = 0
    threads:
        3
    script:
        'scripts/starcode.py'

def get_sum_input(config, wildcards):
    data_type = wildcards.data_type
    name=wildcards.name
    bio= wildcards.bio
    rep_n = len(config['starcode_params']['pick'][data_type][name][bio])
    return(['%s/%s/%s_%s_r%i.raw.count' % (wildcards.outdir, data_type,
                                           name, bio,  i+1)
            for i in range(0, rep_n)])


rule sum_raw_count:
    input:
        lambda wildcards: get_sum_input(config, wildcards)
    output:
        '{outdir}/{data_type}/{name}_{bio}.raw.count'
    wildcard_constraints:
        name="[^_]+",
        bio="[^_]+"
    shell:
        "awk -vOFS='\t' "
        "'{{"
        "   arr[$1] += $2"
        "}} END {{"
        "   for (bc in arr){{"
        "       print bc, arr[bc]"
        "}}}}' {input} > {output}"

def get_starcode_input(config, wildcards):
    pick_dict = config['starcode_params']['pick'][wildcards.data_type]
    bio = pick_dict[wildcards.name][wildcards.bio]
    pick = bio[int(wildcards.rep) - 1]
    param_dict = config['starcode_params']['options'][pick]
    other = param_dict['other']
    yield('%s/%s/%s_%s_r%s.raw.count' % (wildcards.outdir, wildcards.data_type,
                                         wildcards.name, wildcards.bio,
                                         wildcards.rep))
    if config['starcode_params']['options'][pick]['use_other']:
        yield('%s/%s/%s_%s.starcode.count' % (wildcards.outdir, other,
                                              wildcards.name, wildcards.bio))

def get_starcode_params(config, wildcards):
    params = config['starcode_params']
    bio = params['pick'][wildcards.data_type][wildcards.name][wildcards.bio]
    pick = bio[int(wildcards.rep) - 1]
    return(params['options'][pick])

rule starcode:
    input:
        lambda wildcards: get_starcode_input(config, wildcards)
    output:
        gen = '{outdir}/{data_type}/{name}_{bio}_r{rep}.starcode.count',
        mut = '{outdir}/{data_type}/{name}_{bio}_r{rep}.genuine.cut',
        count = '{outdir}/{data_type}/{name}_{bio}_r{rep}.count.cut',
    params:
        param_dict=lambda wildcards: get_starcode_params(config, wildcards),
        not_other = '{outdir}/{data_type}/{name}_{bio}_r{rep}.in_ref.cut',
        not_this = '{outdir}/{data_type}/{name}_{bio}_r{rep}.in_this.cut'
    threads:
        4
    script:
        'scripts/starcode.py'


rule count_barcode:
    input:
        '{outdir}/{name}_{bio}_r{rep}.barcode.txt.gz'
    output:
        '{outdir}/{name}_{bio}_r{rep}.raw.count'
    wildcard_constraints:
        rep="\d+"
    shell:
        "{path}/scripts/count_barcode.sh {input} > {output}"

###############################################################################
##+++++++++++++++++++++++++++++++ parse reads +++++++++++++++++++++++++++++++##
###############################################################################

# rule parse_single:
#     input:
#         '{outdir}/raw_data/{data_type}/{name}_{bio}_r{rep}.fastq.gz',
#     output:
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.fastq.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.barcode.txt.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.statistics.txt',
#         structure = '{outdir}/{data_type}/{name}_{bio}_r{rep}.structure.txt'
#     log:
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.structure.txt'
#     # wildcard_constraints:
#     #     outdir="[^/]+"
#     params:
#         data_type = '{data_type}',
#         outdir = OUTDIR,
#         config = config['structure'],
#         name = '{name}_{bio}_r{rep}',
#         parser = config['parser']
#     run:
#         pick = config['pick'][params.data_type]
#         structure = config['options'][pick]
#         with open(output.structure, 'w') as f:
#             f.write(params.structure)
#         shell('{params.parser} -r -l {log} -b {data_type}/'
#               '{wildcards.name}_{wildcards.bio}_r{wildcards.rep} '
#               '{input} {output.structure} {params.outdir}')
#
# rule parse_paired:
#     input:
#         '{outdir}/raw_data/{data_type}/{name}_{bio}_r{rep}.1.fastq.gz',
#         '{outdir}/raw_data/{data_type}/{name}_{bio}_r{rep}.2.fastq.gz'
#     output:
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.1.fastq.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.2.fastq.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.barcode.txt.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.statistics.txt',
#         structure = '{outdir}/{data_type}/{name}_{bio}_r{rep}.structure.txt'
#     log:
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.structure.txt'
#     # wildcard_constraints:
#     #     outdir="[^/]+"
#     params:
#         data_type = '{data_type}',
#         outdir = OUTDIR,
#         config = config['structure'],
#         name = '{name}_{bio}_r{rep}',
#         parser = config['parser']
#     run:
#         pick = config['pick'][params.data_type]
#         structure = config['options'][pick]
#         with open(output.structure, 'w') as f:
#             f.write(params.structure)
#         shell('{params.parser} -r -l {log} -b {data_type}/'
#               '{wildcards.name}_{wildcards.bio}_r{wildcards.rep} '
#               '-p {input[1]} {input[0]}  {output.structure} {params.outdir}')

def get_structure(config, wildcards, datatype):
    pick = config['structure']['pick'][datatype][wildcards.name]
    return(config['structure']['options'][pick])



for data_type in config['paired_end']:
    if config['paired_end'][data_type]:
        rule:
            input:
                '%s/raw_data/%s/{name}_{bio}_r{rep}.1.fastq.gz' % (OUTDIR, data_type),
                '%s/raw_data/%s/{name}_{bio}_r{rep}.2.fastq.gz' % (OUTDIR, data_type)
            output:
                '%s/%s/{name}_{bio}_r{rep}.barcode.txt.gz' % (OUTDIR, data_type),
                '%s/%s/{name}_{bio}_r{rep}.1.fastq.gz' % (OUTDIR, data_type),
                '%s/%s/{name}_{bio}_r{rep}.2.fastq.gz' % (OUTDIR, data_type),
                '%s/%s/{name}_{bio}_r{rep}.statistics.txt' % (OUTDIR, data_type),
                structure = '%s/%s/{name}_{bio}_r{rep}.structure.txt' % (OUTDIR, data_type)
            log:
                '%s/%s/{name}_{bio}_r{rep}_parser.log' % (OUTDIR, data_type)
            wildcard_constraints:
                outdir="[^/]+",
                rep="\d+"
            params:
                struct_dict= config['structure'],
                outdir = OUTDIR,
                name = '{name}',
                parser = config['parser'],
                data_type = data_type
            run:
                pick = params.struct_dict['pick'][params.data_type][params.name]
                structure = params.struct_dict['options'][pick].replace('\\', '')
                with open(output.structure, 'w') as f:
                    f.write(structure)
                shell('{params.parser} -r -x -a -n 1000000000 -l {log} '
                      '-p {input[1]} -b {params.data_type}/'
                      '{wildcards.name}_{wildcards.bio}_r{wildcards.rep} '
                      '{input[0]} {output.structure} {params.outdir}')
    else:
        rule:
            input:
                '{outdir}/raw_data/%s/{name}_{bio}_r{rep}.fastq.gz' % (data_type),
            output:
                '{outdir}/%s/{name}_{bio}_r{rep}.barcode.txt.gz' % (data_type),
                '{outdir}/%s/{name}_{bio}_r{rep}.statistics.txt' % (data_type),
                structure = '{outdir}/%s/{name}_{bio}_r{rep}.structure.txt' % (data_type)
            log:
                '{outdir}/%s/{name}_{bio}_r{rep}_parser.log' % (data_type)
            params:
                struct_dict= config['structure'],
                outdir = OUTDIR,
                name = '{name}',
                parser = config['parser'],
                data_type = data_type
            run:
                pick = params.struct_dict['pick'][params.data_type][params.name]

                structure = params.struct_dict['options'][pick].replace('\\', '')
                with open(output.structure, 'w') as f:
                    f.write(structure)
                shell('{params.parser} -r -x -l {log} -b {params.data_type}/'
                      '{wildcards.name}_{wildcards.bio}_r{wildcards.rep} '
                      '{input} {output.structure} {params.outdir}')


def get_input(config, input_label):
    name_list = config['input_file']['options'][input_label]
    indir = config['indir']
    return('/'.join((indir, name)) for name in name_list)

def get_raw_output(config, input_dict, input_label):
    out_list = []
    for data_type, name, bio, rep in input_dict[input_label]:
        outdir = config['outdir']
        if len(config['input_file']['options'][input_label]) == 2:
            output = ['%s/raw_data/%s/%s_%s_r%i.%i.fastq.gz' %
                      (outdir, data_type, name, bio, rep + 1, i)
                      for i in (1,2)]
        else:
            output = ['%s/raw_data/%s/%s_%s_r%i.fastq.gz' % (outdir, data_type,
                                                             name, bio,
                                                             rep + 1)]
        out_list.extend(output)
    return(out_list)

if 'input_file' in config:
    input_dict = {input_label:[] for input_label in config['input_file']['options']}
    for data_type in config['input_file']['pick']:
        sample_dict = config['input_file']['pick'][data_type]
        for name in sample_dict:
            for bio in sample_dict[name]:
                rep_list = sample_dict[name][bio]
                for i in range(0, len(rep_list)):
                    input_dict[rep_list[i]].append([data_type, name, bio, i])

    for input_label in config['input_file']['options'].keys():
        rule:
            input:
                get_input(config, input_label)
            output:
                get_raw_output(config, input_dict, input_label),
                structure = '%s/raw_data/%s_structure.txt' % (OUTDIR, input_label)
            params:
                input_label=input_label,
                structure=config['structure']['options']['index'],
                outdir = OUTDIR,
                index_dict = config['index'],
                parser=config['parser']
            log:
                '%s/raw_data/%s_parser.log' % (OUTDIR, input_label)
            run:
                index_list = []
                for data_type, name, bio, rep in input_dict[params.input_label]:
                    label = '%s/%s_%s_r%i' % (data_type, name, bio, rep + 1)
                    index = config['index'][data_type][name][bio][rep]
                    index_list.append(':'.join([label, index]))
                index_vec = ','.join(index_list)
                structure = params.structure % index_vec
                with open(output.structure, 'w') as f:
                    f.write(structure)
                if len(input) == 1:
                    shell('{params.parser} -a -n 10000000000 -r -l {log} '
                          '{input} {output.structure} '
                          '{params.outdir}/raw_data/')
                else:
                    shell('{params.parser} -a -n 10000000000 -r -l {log} '
                          '-p {input[1]} {input[0]} '
                          '{output.structure} '
                          '{params.outdir}/raw_data/')


def get_demulti_single(config, wildcards):
    demulti_dict = config['demultiplexed_input']
    input_list = demulti_dict[wildcards.datatype][wildcards.name][wildcards.bio]
    input_file = input_list[int(wildcards.rep) - 1]
    return('%s/%s' % (config['demultiplexed_indir'], input_file))

rule input_demulti_single:
    input:
        lambda wildcards: get_demulti_single(config, wildcards)
    output:
        '{outdir}/raw_data/{datatype}/{name}_{bio}_r{rep}.fastq.gz'
    wildcard_constraints:
        rep="\d+"
    shell:
        "ln {input} {output}"

def get_demulti_pair(config, wildcards):
    demulti_dict = config['demultiplexed_input']
    input_list = demulti_dict[wildcards.datatype][wildcards.name][wildcards.bio]
    input_file = input_list[int(wildcards.rep) - 1][int(wildcards.pair) - 1]
    return('%s/%s' % (config['demultiplexed_indir'], input_file))

rule input_demulti_paired:
    input:
        lambda wildcards: get_demulti_pair(config, wildcards)
    output:
        '{outdir}/raw_data/{datatype}/{name}_{bio}_r{rep}.{pair}.fastq.gz'
    wildcard_constraints:
        rep="\d+",
        pair="\d+"
    shell:
        "ln {input} {output}"
