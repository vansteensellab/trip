import getpass
import datetime
import inspect
import os
import re

filename = inspect.getframeinfo(inspect.currentframe()).filename
path = os.path.dirname(os.path.abspath(filename))


# user = getpass.getuser()
# date = datetime.datetime.now()
# date = '%i%0.2i%0.2i' % (date.year, date.month, date.day)
# OUTDIR = ''.join((user[0], user[2], date, '_', config["dir_suffix"]))
OUTDIR = config['outdir']
if 'extract' in config:
    TIMING = glob_wildcards(config["extract"]["timing"])[0]
# TYPE_LIST = ['mapping', 'gDNA', 'cDNA', 'spike']
# TYPE_LIST = [read_type for read_type in TYPE_LIST if read_type in config]

# if 'groups' in config:
#     group_name_vec = [group[0] for group in config['groups']]
#     replicate_dict = {}
#     if 'replicate' in group_name_vec:
#         index = group_name_vec.index('replicate')
#         for name in config['input_file']['gDNA'].keys():
#             if 'spike' in config['input_file']:
#                 file_name = '%s/cDNA/%s.cpm.gDNA/spike' % (OUTDIR, name)
#             else:
#                 file_name = '%s/cDNA/%s.cpm.gDNA' % (OUTDIR, name)
#             name_split = name.split('_')
#             if index < len(name_split):
#                 name_vec = [name_split[i] for i in range(0, len(name_split))
#                             if i != index]
#                 mean_name = '_'.join(name_vec)
#                 if mean_name in replicate_dict:
#                     replicate_dict[mean_name].append(file_name)
#                 else:
#                     replicate_dict[mean_name] = [file_name]

def get_input(index_dict, datatype, outdir):
    if datatype == 'all':
        for datatype in index_dict:
            for name in index_dict[datatype]:
                for bio in index_dict[datatype][name]:
                    for i in range(0, len(index_dict[datatype][name][bio])):
                        if datatype == 'mapping':
                            yield('%s/%s/%s_%s_r%i.2.table' % (outdir,
                                                                 datatype,
                                                                 name,
                                                                 bio, i + 1))
                        elif datatype == 'cDNA':
                            yield('%s/%s/%s_%s_r%i.normalized' % (outdir,
                                                                  datatype,
                                                                  name,
                                                                  bio, i + 1))

                        else:
                            yield('%s/%s/%s_%s_r%i.raw.count' % (outdir,
                                                                  datatype,
                                                                  name,
                                                                  bio, i + 1))
    else:
        for name in index_dict[datatype]:
            for bio in index_dict[datatype][name]:
                for i in range(0, len(index_dict[datatype][name][bio])):
                    if datatype == 'mapping':
                        yield('%s/%s/%s_%s_r%i.2.table' % (outdir, datatype,
                                                              name, bio, i + 1))
                    else:
                        yield('%s/%s/%s_%s_r%i.normalized' % (outdir, datatype,
                                                              name, bio, i + 1))


rule test:
    input:
        get_input(config['index'], 'all', OUTDIR)

rule all:
    input:
        expand('{outdir}/cDNA/{name}.normalized', outdir=OUTDIR,
               name=config['index']['cDNA'].keys())



rule parse_sam:
    input:
        bam='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.bam',
        count='{outdir}/mapping/{name}_{bio}_r{rep}.starcode.count',
        mutated='{outdir}/mapping/{name}_{bio}_r{rep}.genuine.cut'
    output:
        bed='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.bed',
        table='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.table',
        stats='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.parse_stat.table',
        length='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.length.table',
        remap_fq='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.remap.fastq.gz',
        remap='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.remap.bam'
    wildcard_constraints:
        name="[^_]+",
        bio="[^_]+",
        num="\d+",
        rep="\d+"
    params:
        bowtie_index = config['bowtie']['index'],
        options = lambda wildcards: config['bowtie']['options'][wildcards.num],
        max_dist = lambda wildcards: config['max_dist'][wildcards.num]
    threads: 10
    script:
        'scripts/parse_sam.py'


rule align:
    input:
        '{outdir}/mapping/{name}_{bio}_r{rep}.{num}.fastq.gz'
    output:
        bam='{outdir}/mapping/{name}_{bio}_r{rep}.{num}.bam'
    params:
        bowtie_index=config['bowtie']['index'],
        options=config['bowtie']['options'],
        num='{num}'
    wildcard_constraints:
        num="\d+",
        rep="\d+"
    threads: 10
    log:
        '{outdir}/mapping/align.{name}.{num}.log'
    run:
        gunzip = "gunzip -c {input}"
        ## filter for read length
        awk = ("awk '{{"
               "       step=NR%4;"
               "       if (step==0 && length(a[2])>6){{"
               "           for (i in a){{"
               "               print a[i]"
               "           }}"
               "           print $0;"
               "           hit+=1;"
               "       }} else if (step!=0){{"
               "           a[step]=$0;"
               "       }} else {{"
               "           short+=1"
               "       }}"
               "}} END {{"
               "print \"filtering before mapping with bowtie2:\" > \"{log}\"; "
               "printf \"%i\\treads; of these:\\n\", hit+short > \"{log}\"; "
               "printf \"  %i (%2.2f%%) were long enough (> 6bp)\\n\", hit, hit/(hit+short)*100 > \"{log}\"; "
               "printf \"  %i (%2.2f%%) were too short (<= 6bp)\\n\\n\", short, short/(hit+short)*100 > \"{log}\"; "
               "print \"stats from bowtie2:\" > \"{log}\"; "
               "}}'")
        options = ' '.join(params.options[params.num])
        bowtie = 'bowtie2 -p {threads} %s -x %s -U - ' % (options,
                                                          params.bowtie_index)
        samToBam = 'samtools view -@ {threads} -Sb - 1> {output.bam} 2>> {log}'
        shell('%s | %s | %s | %s' % (gunzip, awk, bowtie, samToBam))


rule split_mapping:
    input:
        bam=expand('{outdir}/mapping/{{num}}.bam', outdir=OUTDIR)
    output:
        '{outdir}/mapping/{num}.genuine.bam'
        '{outdir}/mapping/{num}.genuine.cut.bam'
        '{outdir}/mapping/{num}.unmapped.bam'


###############################################################################
##+++++++++++++++++++++++++++++ mean expression +++++++++++++++++++++++++++++##
###############################################################################


# rule mean_exp:
#     input:
#         lambda wildcards: replicate_dict[wildcards.mean_name]
#     output:
#         '%s/cDNA/{mean_name}.mean',
#         '%s/cDNA/{mean_name}.mean.cut'
#     run:
#         cpm_dict = {}
#         mean_file = open('{output[0]}', 'w')
#         mean_cut_file = open('{output[1]}', 'w')
#         for input_file in snakemake.input:
#             with open(input_file) as file_in:
#                 for line in file_in.readlines():
#                     norm_cpm, barcode = line.strip().split()
#                     if barcode in cpm_dict:
#                         cpm_dict[barcode][input_file] = float(norm_cpm)
#                     else:
#                         cpm_dict[barcode] = {input_file: float(norm_cpm)}
#         for barcode in cpm_dict:
#             if len(cpm_dict[barcode]) == len(snakemake.input):
#                 mean = sum(cpm_dict[barcode].values())/len(snakemake.input)
#                 mean_file.write('%f\t%s' % (mean, barcode))
#         mean_file.close()
#         mean_cut_file.close()




###############################################################################
##++++++++++++++++++++++ calculate counts per million +++++++++++++++++++++++##
###############################################################################
#
# rule cpm:
#     input:
#         expand('{outdir}/{read_type}.{{name}}.starcode.count', outdir=OUTDIR,
#                read_type = ('cDNA', 'gDNA', 'spike'))
#     output:
#         '{outdir}/{read_type}.{name}.cpm'
#     shell:
#         "awk '{{arr[$2] = $1; sum += $1}}"
#         "END{{for (bc in arr){{print arr[bc]/sum*1000000\"\t\"bc}}}}'"
#         "< {input} > {output}"

if 'spike' in config['input_file']:
    rule normalize_mean_expression:
        input:
            expand('{outdir}/cDNA/{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/gDNA/{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/spike/{{name}}.starcode.count', outdir=OUTDIR)
        output:
            '{outdir}/cDNA/{name}.normalized'
        params:
            path
        shell:
            'Rscript {params}/scripts/normalize.R {input} {output}'
else:
    rule normalize_mean_expression:
        input:
            expand('{outdir}/cDNA/{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/gDNA/{{name}}.starcode.count', outdir=OUTDIR)
        output:
            '{outdir}/cDNA/{name}.normalized'
        params:
            path
        shell:
            'Rscript {params}/scripts/normalize.R {input} {output}'

rule normalize_polyA:
    input:
        lambda wildcards: config['input_file']['polyA'][wildcards.name][0],
        expand('{outdir}/polyA/{{name}}.starcode.count', outdir=OUTDIR)
    output:
        '{outdir}/polyA/{name}.normalized'
    run:
        print(input[0])
        print(input[1])
        shell(("total_reads=$(($(gunzip -c {input[0]} | wc -l) / 4));"
               "awk -v total_reads=\"$total_reads\" '{{"
               "    norm=$2/total_reads * 1000000;"
               "    print $0\"\t\"norm"
               "}}' {input[1]} > {output}"))



###############################################################################
##++++++++++++++++++++++++ select genuine barcodes ++++++++++++++++++++++++++##
###############################################################################

rule starcode_ref:
    input:
        '{outdir}/{data_type}/{name}_{bio}.raw.count'
    output:
        gen='{outdir}/{data_type}/{name}_{bio}.starcode.count',
        mut='{outdir}/{data_type}/{name}_{bio}.genuine.cut',
        count='{outdir}/{data_type}/{name}_{bio}.count.cut'
    wildcard_constraints:
        name="[^_]+",
        bio="[^_]+"
    params:
        lev_dist = config['lev_dist'],
        use_other = False,
        count = 0
    threads:
        3
    script:
        'scripts/starcode.py'

def get_sum_input(config, wildcards):
    rep_n = len(config['index'][wildcards.data_type]
                      [wildcards.name][wildcards.bio])
    return(['%s/%s/%s_%s_r%i.raw.count' % (wildcards.outdir,
                                           wildcards.data_type,
                                           wildcards.name,
                                           wildcards.bio,  i+1)
            for i in range(0, rep_n)])

rule sum_raw_count:
    input:
        lambda wildcards: get_sum_input(config, wildcards)
    output:
        '{outdir}/{data_type}/{name}_{bio}.raw.count'
    wildcard_constraints:
        name="[^_]+",
        bio="[^_]+"
    shell:
        "awk -vOFS='\t' "
        "'{{"
        "   arr[$1] += $2"
        "}} END {{"
        "   for (bc in arr){{"
        "       print bc, arr[bc]"
        "}}}}' {input} > {output}"

def get_starcode_input(config, wildcards):
    params = config['starcode_params']['pick']
    print(params[wildcards.data_type].keys())
    bio = params[wildcards.data_type][wildcards.name][wildcards.bio]
    pick = bio[int(wildcards.rep) - 1]
    return('%s/%s/%s_%s.starcode.count' %
             (wildcards.outdir,
              config['starcode_params']['options'][pick]['other'],
              wildcards.name, wildcards.bio))

def get_starcode_params(config, wildcards):
    params = config['starcode_params']
    bio = params['pick'][wildcards.data_type][wildcards.name][wildcards.bio]
    pick = bio[int(wildcards.rep) - 1]
    return(params['options'][pick])

rule starcode:
    input:
        '{outdir}/{data_type}/{name}_{bio}_r{rep}.raw.count',
        lambda wildcards: get_starcode_input(config, wildcards)
    output:
        gen = '{outdir}/{data_type}/{name}_{bio}_r{rep}.starcode.count',
        mut = '{outdir}/{data_type}/{name}_{bio}_r{rep}.genuine.cut',
        count = '{outdir}/{data_type}/{name}_{bio}_r{rep}.count.cut',
        not_other = '{outdir}/{data_type}/{name}_{bio}_r{rep}.in_ref.cut',
        not_this = '{outdir}/{data_type}/{name}_{bio}_r{rep}.in_this.cut'
    params:
        param_dict=lambda wildcards: get_starcode_params(config, wildcards),
    threads:
        4
    script:
        'scripts/starcode.py'


rule count_barcode:
    input:
        '%s/{file_base}/{name}_{bio}_r{rep}.barcode.txt.gz' % OUTDIR
    output:
        '%s/{file_base}/{name}_{bio}_r{rep}.raw.count' % OUTDIR
    wildcard_constraints:
        rep="\d+"
    shell:
        "{path}/scripts/count_barcode.sh {input} > {output}"

###############################################################################
##+++++++++++++++++++++++++++++++ parse reads +++++++++++++++++++++++++++++++##
###############################################################################

# rule parse_single:
#     input:
#         '{outdir}/raw_data/{data_type}/{name}_{bio}_r{rep}.fastq.gz',
#     output:
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.fastq.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.barcode.txt.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.statistics.txt',
#         structure = '{outdir}/{data_type}/{name}_{bio}_r{rep}.structure.txt'
#     log:
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.structure.txt'
#     # wildcard_constraints:
#     #     outdir="[^/]+"
#     params:
#         data_type = '{data_type}',
#         outdir = OUTDIR,
#         config = config['structure'],
#         name = '{name}_{bio}_r{rep}',
#         parser = config['parser']
#     run:
#         pick = config['pick'][params.data_type]
#         structure = config['options'][pick]
#         with open(output.structure, 'w') as f:
#             f.write(params.structure)
#         shell('{params.parser} -r -l {log} -b {data_type}/'
#               '{wildcards.name}_{wildcards.bio}_r{wildcards.rep} '
#               '{input} {output.structure} {params.outdir}')
#
# rule parse_paired:
#     input:
#         '{outdir}/raw_data/{data_type}/{name}_{bio}_r{rep}.1.fastq.gz',
#         '{outdir}/raw_data/{data_type}/{name}_{bio}_r{rep}.2.fastq.gz'
#     output:
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.1.fastq.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.2.fastq.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.barcode.txt.gz',
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.statistics.txt',
#         structure = '{outdir}/{data_type}/{name}_{bio}_r{rep}.structure.txt'
#     log:
#         '{outdir}/{data_type}/{name}_{bio}_r{rep}.structure.txt'
#     # wildcard_constraints:
#     #     outdir="[^/]+"
#     params:
#         data_type = '{data_type}',
#         outdir = OUTDIR,
#         config = config['structure'],
#         name = '{name}_{bio}_r{rep}',
#         parser = config['parser']
#     run:
#         pick = config['pick'][params.data_type]
#         structure = config['options'][pick]
#         with open(output.structure, 'w') as f:
#             f.write(params.structure)
#         shell('{params.parser} -r -l {log} -b {data_type}/'
#               '{wildcards.name}_{wildcards.bio}_r{wildcards.rep} '
#               '-p {input[1]} {input[0]}  {output.structure} {params.outdir}')


for data_type, pick in config['structure']['pick'].items():
    if config['structure']['paired_end'][pick]:
        rule:
            input:
                '{outdir}/raw_data/%s/{name}_{bio}_r{rep}.1.fastq.gz' % (data_type),
                '{outdir}/raw_data/%s/{name}_{bio}_r{rep}.2.fastq.gz' % (data_type)
            output:
                '{outdir}/%s/{name}_{bio}_r{rep}.barcode.txt.gz' % (data_type),
                '{outdir}/%s/{name}_{bio}_r{rep}.1.fastq.gz' % (data_type),
                '{outdir}/%s/{name}_{bio}_r{rep}.2.fastq.gz' % (data_type),
                '{outdir}/%s/{name}_{bio}_r{rep}.statistics.txt' % (data_type),
                structure = '{outdir}/%s/{name}_{bio}_r{rep}.structure.txt' % (data_type)
            log:
                '{outdir}/%s/{name}_{bio}_r{rep}_parser.log' % (data_type)
            wildcard_constraints:
                outdir="[^/]+",
                rep="\d+"
            params:
                structure= config['structure']['options'][data_type],
                outdir = OUTDIR,
                name = '{name}_{bio}_r{rep}',
                parser = config['parser'],
                data_type = data_type
            run:
                structure = params.structure.replace('\\', '')
                with open(output.structure, 'w') as f:
                    f.write(structure)
                shell('{params.parser} -r -x -a -n 1000000000 -l {log} '
                      '-p {input[1]} -b {params.data_type}/'
                      '{wildcards.name}_{wildcards.bio}_r{wildcards.rep} '
                      '{input[0]} {output.structure} {params.outdir}')
    else:
        rule:
            input:
                '{outdir}/raw_data/%s/{name}_{bio}_r{rep}.fastq.gz' % (data_type),
            output:
                '{outdir}/%s/{name}_{bio}_r{rep}.barcode.txt.gz' % (data_type),
                '{outdir}/%s/{name}_{bio}_r{rep}.statistics.txt' % (data_type),
                structure = '{outdir}/%s/{name}_{bio}_r{rep}.structure.txt' % (data_type)
            log:
                '{outdir}/%s/{name}_{bio}_r{rep}_parser.log' % (data_type)
            params:
                structure = config['structure']['options'][pick],
                outdir = OUTDIR,
                parser = config['parser'],
                data_type = data_type
            run:
                with open(output.structure, 'w') as f:
                    f.write(params.structure)
                shell('{params.parser} -r -x -l {log} -b {params.data_type}/'
                      '{wildcards.name}_{wildcards.bio}_r{wildcards.rep} '
                      '{input} {output.structure} {params.outdir}')


def get_input(config, input_label):
    name_list = config['input_file']['options'][input_label]
    indir = config['indir']
    return('/'.join((indir, name)) for name in name_list)

def get_raw_output(config, input_dict, input_label):
    out_list = []
    for data_type, name, bio, rep in input_dict[input_label]:
        outdir = config['outdir']
        if len(config['input_file']['options'][input_label]) == 2:
            output = ['%s/raw_data/%s/%s_%s_r%i.%i.fastq.gz' %
                      (outdir, data_type, name, bio, rep + 1, i)
                      for i in (1,2)]
        else:
            output = ['%s/raw_data/%s/%s_%s_r%i.fastq.gz' % (outdir, data_type,
                                                             name, bio,
                                                             rep + 1)]
        out_list.extend(output)
    return(out_list)

input_dict = {input_label:[] for input_label in config['input_file']['options']}
for data_type in config['input_file']['pick']:
    sample_dict = config['input_file']['pick'][data_type]
    for name in sample_dict:
        for bio in sample_dict[name]:
            rep_list = sample_dict[name][bio]
            for i in range(0, len(rep_list)):
                input_dict[rep_list[i]].append([data_type, name, bio, i])

for input_label in config['input_file']['options'].keys():
    rule:
        input:
            get_input(config, input_label)
        output:
            get_raw_output(config, input_dict, input_label),
            structure = '%s/raw_data/%s_structure.txt' % (OUTDIR, input_label)
        params:
            input_label=input_label,
            structure=config['structure']['options']['index'],
            outdir = OUTDIR,
            index_dict = config['index'],
            parser=config['parser']
        log:
            '%s/raw_data/%s_parser.log' % (OUTDIR, input_label)
        run:
            index_list = []
            for data_type, name, bio, rep in input_dict[params.input_label]:
                label = '%s/%s_%s_r%i' % (data_type, name, bio, rep + 1)
                index = config['index'][data_type][name][bio][rep]
                index_list.append(':'.join([label, index]))
            index_vec = ','.join(index_list)
            structure = params.structure % index_vec
            with open(output.structure, 'w') as f:
                f.write(structure)
            if len(input) == 1:
                shell('{params.parser} -a -n 10000000000 -r -l {log} '
                      '{input} {output.structure} '
                      '{params.outdir}/raw_data/')
            else:
                shell('{params.parser} -a -n 10000000000 -r -l {log} '
                      '-p {input[1]} {input[0]} '
                      '{output.structure} '
                      '{params.outdir}/raw_data/')
